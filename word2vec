import __future__
import codecs
import glob
import multiprocessing
import os
import pprint
import re
import nltk
import gensim.models.word2vec as w2v
import sklearn.manifold
import numpy as np
import matplotlib as plt
import pandas as pd
import seaborn as sns

# nltk.download('punkt')
# nltk.download('stopwords')
#
# data_files = sorted(glob.glob('./got/data/*.txt'))
# print(data_files)
#
# corpus_raw = u''
#
# for book_filename in data_files:
#     print("Reading '{0}'...".format(book_filename))
#     with codecs.open(book_filename, "r", "utf-8") as book_file:
#         corpus_raw += book_file.read()
#     print("Corpus is now {0} characters long".format(len(corpus_raw)))
#     print()
#
# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
# raw_sentences = tokenizer.tokenize(corpus_raw)
#
#
# def sentence_to_wordlist(raw):
#     clean = re.sub("[^a-zA-Z]", " ", raw)
#     words = clean.split()
#     return words
#
# sentences = []
# for raw_sentence in raw_sentences:
#     if len(raw_sentence) > 0:
#         sentences.append(sentence_to_wordlist(raw_sentence))
#
# print(raw_sentences[5])
# print(sentence_to_wordlist(raw_sentences[5]))
#
# token_count = sum([len(sentence) for sentence in sentences])
# print("The book corpus contains {0:,} tokens".format(token_count))
#
# num_features = 300
# min_word_count = 3
# num_workers = multiprocessing.cpu_count()
# context_size = 7
# downsampling = 1e-3
# seed = 1
#
#
#
# thrones2vec = w2v.Word2Vec(
#     sg=1,
#     seed=seed,
#     workers=num_workers,
#     size=num_features,
#     min_count=min_word_count,
#     window=context_size,
#     sample=downsampling,
# )
#
# thrones2vec.build_vocab(sentences)
# print("Word2Vec vocabulary length:", len(thrones2vec.wv.vocab))
#
# thrones2vec.train(sentences, total_examples=thrones2vec.corpus_count, epochs=thrones2vec.iter)
#
# if not os.path.exists("trained"):
#     os.makedirs("trained")
# thrones2vec.save(os.path.join("trained", "thrones2vec.w2v"))

thrones2vec = w2v.Word2Vec.load(os.path.join("trained", "thrones2vec.w2v"))
print('Started loading "Game of Thrones" model...')


def nearest_similarity_cosmul(start1, end1, end2):
    similarities = thrones2vec.most_similar_cosmul(
        positive=[end2, start1],
        negative=[end1]
    )
    start2 = similarities[0][0]
    print("{start1} is related to {end1}, as {start2} is related to {end2}".format(**locals()))
    output = ("{start1} is related to {end1}, as {start2} is related to {end2}".format(**locals()))
    return output

#print(thrones2vec.most_similar("Stark"))


def ask_about(input_string):
    output = thrones2vec.most_similar(input_string)
    print('You gave me : ' + str(input_string))
    print('I give you ' + str(output))
    return output


def ask():
    print('--1-- us related to --2--, as (ill answer) is related to --3--')
    one = input('1 :')
    two = input('2 :')
    three = input('3 :')
    nearest_similarity_cosmul(one, two, three)
    ask()
print('Finished loading "Game of Thrones" model...')
